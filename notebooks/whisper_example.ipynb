{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5thx0SEBIIN"
   },
   "source": [
    "# Initialisation and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "lib_path = '/home/jovyan/libs'\n",
    "sys.path.insert(0, lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9rvOFi6QJP0z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc, math, traceback, os, datetime, logging\n",
    "from typing import Callable, Union, Optional\n",
    "from functools import partial\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import whisper\n",
    "from whisper.audio import N_FRAMES, N_SAMPLES\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EjUegebBNcX"
   },
   "source": [
    "# GPU RAM Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jobgOLtEeLpx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_cuda_usage(msg: str = \"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"{msg}{torch.cuda.memory_allocated(0)/(1024 ** 3)} GB\")\n",
    "\n",
    "def get_cuda_usage():\n",
    "    return torch.cuda.memory_allocated(0)/(1024 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1zjyFx78LvD-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNEPbtquUG_g"
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aTrPYXKiygaU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    print(\"Model deleted!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zfgWJdLVJRbS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/libs/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"small.en\"\n",
    "\n",
    "model = whisper.load_model(MODEL_NAME).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tolbjGxG7qB8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model.is_multilingual, num_languages=model.num_languages, language=\"en\", task=\"transcribe\")\n",
    "sot_ids = torch.tensor(tokenizer.sot_sequence_including_notimestamps, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8P5xS3UBTfT"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "upnfsCvahcEJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tedlium_path = \"./tedlium\"\n",
    "train_path, validation_path, test_path = f\"{tedlium_path}/train.hf\", f\"{tedlium_path}/validation.hf\", f\"{tedlium_path}/test.hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23967c3e55f24af687077cfd05e04e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_SELECT = 500\n",
    "VALID_SELECT = 150\n",
    "TEST_SELECT = 250\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "tedlium_train = load_from_disk(train_path).with_format(\"torch\").shuffle(seed=SEED).select(range(TRAIN_SELECT))\n",
    "tedlium_validation = load_from_disk(validation_path).with_format(\"torch\").shuffle(seed=SEED).select(range(VALID_SELECT))\n",
    "tedlium_test = load_from_disk(test_path).with_format(\"torch\").shuffle(seed=SEED).select(range(TEST_SELECT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MoAt_Utrb4sT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(ls):\n",
    "    pad_to = max(list(map(lambda x: x[\"audio\"].shape[0], ls)))\n",
    "    return torch.cat(list(map(lambda x: F.pad(x[\"audio\"], (0, pad_to - x[\"audio\"].shape[0])).unsqueeze(0).to(torch.bfloat16), ls)), dim=0)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1 # highly recommended to be 1\n",
    "VALID_BATCH_SIZE = 75\n",
    "\n",
    "train_dataset = DataLoader(tedlium_train, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate)\n",
    "validation_dataset = DataLoader(tedlium_validation, batch_size=VALID_BATCH_SIZE, collate_fn=collate)\n",
    "test_dataset = DataLoader(tedlium_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY8-HSVPBYnZ"
   },
   "source": [
    "# Training Loop Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BzcVuVrg68uE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helpers involving model interaction and loss\n",
    "\n",
    "General flow is Audio Tensor --> Mel Tensor --> model.forward --> Logits --> Get log probabilities\n",
    "\"\"\"\n",
    "\n",
    "def audio_to_mel(audio: torch.Tensor) -> torch.Tensor:\n",
    "    return whisper.pad_or_trim(whisper.log_mel_spectrogram(audio, padding=N_SAMPLES),\n",
    "                              N_FRAMES)\n",
    "\n",
    "def audio_to_mel_batch(audio_batch: torch.Tensor) -> torch.Tensor:\n",
    "    if len(audio_batch.shape) == 1:\n",
    "        audio_batch = audio_batch.unsqueeze(0)\n",
    "    return torch.stack([audio_to_mel(audio) for audio in audio_batch])\n",
    "\n",
    "def mel_to_logits_batch(model: whisper.model.Whisper, mel_batch: torch.Tensor, sot_ids: torch.Tensor) -> torch.Tensor:\n",
    "    sot_ids = sot_ids.unsqueeze(0).expand(mel_batch.size(0), -1).to(device)\n",
    "    return model.forward(mel_batch, sot_ids)\n",
    "\n",
    "def get_loss_batch(logits: torch.Tensor, token_id: torch.Tensor) -> torch.Tensor:\n",
    "    sf = torch.nn.Softmax(dim=1)\n",
    "    log_probs = torch.log(sf(logits))\n",
    "    tgt_probs = log_probs[:,token_id].squeeze()\n",
    "    return -1 * torch.mean(tgt_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lf48eLc8U5Aj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utils to view mel\n",
    "\"\"\"\n",
    "\n",
    "def view_mel(audio: torch.Tensor) -> None:\n",
    "    mel = whisper.log_mel_spectrogram(audio)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(mel)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def mel_image(audio: torch.Tensor, pseudocolor_map: str = \"Blues\") -> torch.Tensor:\n",
    "    image = whisper.log_mel_spectrogram(audio.squeeze())\n",
    "    cm = colormaps[pseudocolor_map]\n",
    "    return torch.tensor(cm(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OHBIv30toPm3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Other utils\n",
    "\"\"\"\n",
    "\n",
    "# unused\n",
    "# def validate(model: whisper.model.Whisper, snippet: torch.Tensor, validation: torch.Tensor, \n",
    "#              prepare_method, tokenizer: whisper.tokenizer.Tokenizer) -> torch.Tensor:\n",
    "#     with torch.no_grad():\n",
    "#         validation = validation.to(device)\n",
    "#         attacked_data = prepare_method(snippet, validation)\n",
    "#         mel = audio_to_mel_batch(attacked_data)\n",
    "#         logits = mel_to_logits_batch(model, mel, sot_ids)[:,-1,:].squeeze(dim=1)\n",
    "#         loss = get_loss_batch(logits, tokenizer.eot)\n",
    "#         return loss\n",
    "\n",
    "# check if any values in the snippet violate the clamp constraint\n",
    "def violates_clamp(snippet: torch.Tensor, clamp_epsilon: float) -> bool:\n",
    "    if clamp_epsilon:\n",
    "        with torch.no_grad():\n",
    "            return torch.any(torch.logical_or(snippet > clamp_epsilon, snippet < -clamp_epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Audio Modification Helpers\n",
    "- Butterworth High Pass and Low Pass filters\n",
    "- Compressions (Mu Law and Inverse Mu Law for reversibility)\n",
    "\"\"\"\n",
    "\n",
    "def lowpass_filter(audio_tensor: torch.Tensor, cutoff: int, sampling_rate: int = 16_000, order: int = 5) -> torch.Tensor:\n",
    "    b, a = butter(order, cutoff, btype=\"lowpass\", fs=sampling_rate, analog=False)\n",
    "    y = lfilter(b, a, audio_tensor)\n",
    "    return torch.from_numpy(y).to(torch.float32)\n",
    "\n",
    "def highpass_filter(audio_tensor: torch.Tensor, cutoff: int, sampling_rate: int = 16_000, order: int = 5) -> torch.Tensor:\n",
    "    b, a = butter(order, cutoff, btype=\"highpass\", fs=sampling_rate, analog=False)\n",
    "    y = lfilter(b, a, audio_tensor)\n",
    "    return torch.from_numpy(y).to(torch.float32)\n",
    "\n",
    "################################################\n",
    "\n",
    "def mu_law(audio_tensor: torch.Tensor, mu: int = 255) -> torch.Tensor:\n",
    "    sign = torch.where(audio_tensor >= 0, 1, -1)\n",
    "    return sign * torch.log(1 + mu * torch.abs(audio_tensor)) / math.log(1 + mu)\n",
    "\n",
    "def inv_mu_law(audio_tensor: torch.Tensor, mu: int = 255) -> torch.Tensor:\n",
    "    sign = torch.where(audio_tensor >= 0, 1, -1)\n",
    "    return sign * (torch.tensor([mu + 1]).pow(torch.abs(audio_tensor)) - 1) / mu\n",
    "\n",
    "def mu_comp_decomp(audio_tensor: torch.Tensor, mu: int = 255) -> torch.Tensor:\n",
    "    return inv_mu_law(mu_law(audio_tensor, mu), mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Audio saving, loading and playing\n",
    "\"\"\"\n",
    "def save_audio(audio_tensor: torch.Tensor, filename: str = \"test.wav\", sample_rate: int = 16_000) -> None:\n",
    "    if len(audio_tensor.shape) == 1:\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "    elif audio_tensor.size(0) != 1:\n",
    "        raise ValueError(f\"audio_tensor must be of dims (1, n), current dims are {audio_tensor.shape}\")\n",
    "    torchaudio.save(filename, audio_tensor.detach().to(\"cpu\"), sample_rate)\n",
    "\n",
    "def load_audio(filename: str = \"test.wav\") -> torch.Tensor:\n",
    "    return torchaudio.load(filename)[0]\n",
    "\n",
    "def play_audio(param: Union[torch.Tensor, str], filename: str = \"test.wav\", sample_rate: int = 16_000):\n",
    "    if isinstance(param, torch.Tensor):\n",
    "        save_audio(param, filename, sample_rate)\n",
    "    display(Audio(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK7qsqAVrTO3"
   },
   "source": [
    "# Attack Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "j7njJfPNpVSG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "every attack preparation method must:\n",
    "1. inherit from PrepareMethod (an abstract class)\n",
    "2. Override __call__\n",
    "\n",
    "* each example can either be a batch of audio tensors (M, N) or a single audio tensor (1, N)\n",
    "* the snippet must be of shape (1, N)\n",
    "\"\"\"\n",
    "class PrepareMethod(ABC):\n",
    "    def __init__(self, snippet_size: tuple, name: str):\n",
    "        assert len(snippet_size) == 2, f\"Snippet must have 2 dimensions, currently has {len(snippet_size)} dims\"\n",
    "        assert snippet_size[0] == 1, f\"Snippet must be of shape (1, N), currently of shape {snippet_size}\"\n",
    "        self.snippet_size = snippet_size\n",
    "        self.name = name\n",
    "    \n",
    "    def check_dims(self, snippet, example, desired_dims=2):\n",
    "        offenders = \"\"\n",
    "        if len(snippet.shape) != desired_dims:\n",
    "            offenders += f\"Need snippet (dims {len(snippet.shape)}, shape {snippet.shape}) to be of dims {desired_dims}\\n\"\n",
    "        if len(example.shape) != desired_dims:\n",
    "            offenders += f\"Need example (dims {len(example.shape)}, shape {example.shape}) to be of dims {desired_dims}\\n\"\n",
    "        if offenders:\n",
    "            raise ValueError(offenders.strip())\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, snippet, example):\n",
    "        pass\n",
    "    \n",
    "#################################\n",
    "\n",
    "class PrepareFront(PrepareMethod):\n",
    "    def __init__(self):\n",
    "        super().__init__((1, 10240), \"prepare_front\")\n",
    "    \n",
    "    def __call__(self, snippet, example):\n",
    "        self.check_dims(snippet, example)\n",
    "        snippet = snippet.repeat(example.size(0), 1)\n",
    "        return torch.cat([snippet, example], dim=1)\n",
    "\n",
    "#################################\n",
    "\n",
    "class PrepareOverlay(PrepareMethod):\n",
    "    def __init__(self):\n",
    "        super().__init__((1, 480_000), \"prepare_overlay\")\n",
    "    \n",
    "    def __call__(self, snippet, example):\n",
    "        self.check_dims(snippet, example)\n",
    "        example = F.pad(example, (0, snippet.size(1) - example.size(1)), \"constant\", 0)\n",
    "        snippet = snippet.repeat(example.size(0), 1)\n",
    "        return snippet + example\n",
    "    \n",
    "#################################\n",
    "\n",
    "class PrepareOverlayFront(PrepareMethod):\n",
    "    def __init__(self, snippet_size):\n",
    "        super().__init__(snippet_size, \"prepare_overlay_front\")\n",
    "    \n",
    "    def __call__(self, snippet, example):\n",
    "        self.check_dims(snippet, example)\n",
    "        snippet = F.pad(snippet, (0, example.size(1) - snippet.size(1)), \"constant\", 0)\n",
    "        snippet = snippet.repeat(example.size(0), 1)\n",
    "        return snippet + example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mu-Law Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "                         \n",
    "class PrepareFrontMu(PrepareMethod):\n",
    "    def __init__(self):\n",
    "        super().__init__((1, 480_000), \"prepare_overlay_mu\")\n",
    "    \n",
    "    def __call__(self, snippet, example):\n",
    "        self.check_dims(snippet, example)\n",
    "        example = F.pad(example, (0, snippet.size(1) - example.size(1)), \"constant\", 0)\n",
    "        snippet = snippet.repeat(example.size(0), 1)\n",
    "        return mu_law(torch.cat([snippet, example], dim=1))\n",
    "\n",
    "#################################\n",
    "                         \n",
    "class PrepareOverlayMu(PrepareMethod):\n",
    "    def __init__(self):\n",
    "        super().__init__((1, 480_000), \"prepare_overlay_mu\")\n",
    "    \n",
    "    def __call__(self, snippet, example):\n",
    "        self.check_dims(snippet, example)\n",
    "        example = F.pad(example, (0, snippet.size(1) - example.size(1)), \"constant\", 0)\n",
    "        snippet = snippet.repeat(example.size(0), 1)\n",
    "        return mu_law(snippet + example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass-Filter Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Unused Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unused\n",
    "def prepare_middle(snippet, example):\n",
    "    check_dims(snippet, \"snippet\")\n",
    "    check_dims(example, \"example\")\n",
    "    fragment1, fragment2 = torch.tensor_split(example, 2, dis=1)\n",
    "    snippet = snippet.repeat(example.size(0), 1)\n",
    "    return torch.cat([fragment1, snippet, fragment2])\n",
    "\n",
    "def prepare_back(snippet, example):\n",
    "    check_dims(snippet, \"snippet\")\n",
    "    check_dims(example, \"example\")\n",
    "    snippet = snippet.repeat(example.size(0), 1)\n",
    "    return torch.cat([example, snippet], dim=1)\n",
    "\n",
    "def prepare_overlay(snippet, example):\n",
    "    check_dims(snippet, \"snippet\")\n",
    "    check_dims(example, \"example\")\n",
    "    snippet = F.pad(snippet, (0, example.size(1) - snippet.size(1)), \"constant\", 0)\n",
    "    snippet = snippet.repeat(example.size(0), 1)\n",
    "    return snippet + example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEl7cH9fBbZb"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lejmrAxFOy5E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_autograd(model: whisper.model.Whisper,\n",
    "                      train_data: torch.utils.data.DataLoader, valid_data: torch.utils.data.DataLoader,\n",
    "                      prepare_method: PrepareMethod,\n",
    "                      writer: SummaryWriter = None,\n",
    "                      lr: float = 1e-3,\n",
    "                      iter_limit: int = None, mins_limit: int = None, patience: int = None, clamp_epsilon: float = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Runs the training loop and returns learned adversarial snippet.\n",
    "\n",
    "    Args:\n",
    "        model (whisper.model.Whisper)             : Whisper model\n",
    "        train_data (torch.utils.data.DataLoader)  : Training dataset\n",
    "        valid_data (torch.utils.data.DataLoader)  : Validation dataset\n",
    "        prepare_method (PrepareMethod)            : Preprocessing class that combines the adversarial snippet and the training data\n",
    "        writer (SummaryWriter)                    : Writer for the Tensorboard (Default is None)\n",
    "        lr (float)                                : Optimizer Learning Rate (Default is 1e-3)\n",
    "        iter_limit (int)                          : Epoch limit (Default is None)\n",
    "        mins_limit (int)                          : Time limit in minutes (Default is None)\n",
    "        patience (int)                            : Patience for Early Stopping (Default is None)\n",
    "        clamp_epsilon (float)                     : Value for L-Infinity clamping of the snippet. If None, does not clamp (Default is None)\n",
    "\n",
    "    Returns:\n",
    "        snippet (torch.Tensor)                    : Learned adversarial snippet\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialise stuff\n",
    "    torch.autograd.set_detect_anomaly(False)\n",
    "    loss = torch.tensor(np.inf, requires_grad=True)\n",
    "    num_training_batches = len(train_data)\n",
    "    num_valid_batches = len(valid_data)\n",
    "\n",
    "    time_limit = mins_limit * 60 if mins_limit else None\n",
    "\n",
    "    snippet = torch.rand(prepare_method.snippet_size, requires_grad=True, device=device) # adversarial snippet\n",
    "    if clamp_epsilon:\n",
    "        with torch.no_grad():\n",
    "            snippet = snippet * clamp_epsilon\n",
    "    snippet.requires_grad = True\n",
    "\n",
    "    optim = torch.optim.AdamW([snippet], lr=lr, weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.5)\n",
    "\n",
    "    attack_stack, attacked_data, mel, logits, pbar, avg_valid_loss = None, None, None, None, None, None\n",
    "    lowest_valid_loss = np.inf\n",
    "    curr_patience = patience\n",
    "    best_snippet = snippet.detach().clone()\n",
    "\n",
    "    # display attack method and snippet for sanity check\n",
    "    print(f\"Prepare method: {prepare_method.name}\")\n",
    "    print(f\"Snippet initialised to [{torch.min(snippet)}, {torch.max(snippet)}] of size {prepare_method.snippet_size}\")\n",
    "    print(f\"Clamp: {clamp_epsilon}\\nTime Limit (Mins): {mins_limit}\\nEpochs Limit: {iter_limit}\")\n",
    "\n",
    "    # log attack snippet\n",
    "    if writer:\n",
    "        writer.add_image(\"Attack Snippet\", mel_image(snippet.detach().to(\"cpu\")), 0, dataformats=\"HWC\")\n",
    "        writer.flush()\n",
    "\n",
    "    # progress bar\n",
    "    pbar = tqdm(range(1), desc=\"Training\", ncols=0)\n",
    "    itera = 0\n",
    "\n",
    "    # track gpu usage\n",
    "    base_cuda_usage = get_cuda_usage()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            itera += 1\n",
    "            if iter_limit:\n",
    "                iter_limit -= 1\n",
    "                if iter_limit <= 0:\n",
    "                    pbar.set_postfix_str(\"Epoch limit reached! Terminating...\")\n",
    "                    break\n",
    "            if time_limit and pbar.format_dict[\"elapsed\"] >= time_limit:\n",
    "                pbar.set_postfix_str(\"Time limit reached! Terminating...\")\n",
    "                break\n",
    "            if patience and avg_valid_loss:\n",
    "                if curr_patience <= 0:\n",
    "                    pbar.set_postfix_str(\"Patience expired! Terminating...\")\n",
    "                    break\n",
    "            if clamp_epsilon and violates_clamp(snippet, clamp_epsilon):\n",
    "                raise ValueError(\"Snippet values violate clamp constraint!!\")\n",
    "\n",
    "            avg_training_loss = 0\n",
    "            avg_valid_loss = 0\n",
    "            total_cuda_usage_iter = 0\n",
    "\n",
    "            for batch_no, batch in enumerate(train_data):\n",
    "                pbar.set_postfix_str(f\"Iter {itera}, Training Batch {batch_no + 1}/{num_training_batches}\")\n",
    "\n",
    "                batch = batch.to(device)\n",
    "                attacked_data = prepare_method(snippet, batch)\n",
    "\n",
    "                # forward prop\n",
    "                mel = audio_to_mel_batch(attacked_data)\n",
    "                logits = mel_to_logits_batch(model, mel, sot_ids)[:,-1,:].squeeze(dim=1)\n",
    "                loss = get_loss_batch(logits, tokenizer.eot)\n",
    "                \n",
    "                # get training metrics\n",
    "                total_cuda_usage_iter += get_cuda_usage()\n",
    "                avg_training_loss += loss.detach()\n",
    "\n",
    "                # backprop\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                \n",
    "                # clamp snippet\n",
    "                if clamp_epsilon:\n",
    "                    with torch.no_grad():\n",
    "                        snippet.clamp_(min=-clamp_epsilon, max=clamp_epsilon)\n",
    "                        \n",
    "                snippet.requires_grad = True\n",
    "                batch.to(\"cpu\")\n",
    "\n",
    "            avg_training_loss /= len(train_data)\n",
    "\n",
    "            # validation\n",
    "            with torch.no_grad():\n",
    "                for batch_no, v in enumerate(valid_data):\n",
    "                    pbar.set_postfix_str(f\"Iter {itera}, Validation Batch {batch_no + 1}/{num_valid_batches}\")\n",
    "                    v = v.to(device)\n",
    "                    attacked_data_valid = prepare_method(snippet, v)\n",
    "                    mel_valid = audio_to_mel_batch(attacked_data_valid)\n",
    "                    logits_valid = mel_to_logits_batch(model, mel_valid, sot_ids)[:,-1,:].squeeze(dim=1)\n",
    "                    avg_valid_loss += get_loss_batch(logits_valid, tokenizer.eot)\n",
    "            avg_valid_loss /= num_valid_batches\n",
    "            \n",
    "            # track lowest valid loss and save the snippet with lowest valid loss\n",
    "            if avg_valid_loss >= lowest_valid_loss:\n",
    "                curr_patience -= 1\n",
    "            else:\n",
    "                curr_patience = patience\n",
    "                lowest_valid_loss = avg_valid_loss\n",
    "                best_snippet = snippet.detach().clone()\n",
    "\n",
    "            pbar.write(f\"Trng Avg Loss: {avg_training_loss} | Valid Avg Loss: {avg_valid_loss} | Patience: {curr_patience} | LR: {scheduler.get_last_lr()} | Epoch Limit: {iter_limit}\")\n",
    "\n",
    "            if writer:\n",
    "              # log training and validation losses\n",
    "                writer.add_scalar(\"Training average loss\", avg_training_loss, itera)\n",
    "                writer.add_scalar(\"Validation average loss\", avg_valid_loss, itera)\n",
    "\n",
    "                # log GPU RAM usage\n",
    "                if torch.cuda.is_available():\n",
    "                    writer.add_scalar(\"GPU RAM Usage\", total_cuda_usage_iter / num_training_batches - base_cuda_usage, itera)\n",
    "\n",
    "              # log attack snippet and flush\n",
    "                writer.add_image(\"Attack Snippet\", mel_image(snippet.detach().to(\"cpu\")), itera, dataformats=\"HWC\")\n",
    "                writer.flush()\n",
    "            \n",
    "            # LR decay\n",
    "            scheduler.step()\n",
    "            \n",
    "            # refresh pbar to (hopefully) force update of progress bar\n",
    "            pbar.refresh()\n",
    "\n",
    "    except Exception as e:\n",
    "        # need to explicitly close pbar here so traceback can print error\n",
    "        if pbar is not None:\n",
    "            pbar.clear()\n",
    "            pbar.close()\n",
    "            traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # close pbar to free stdout/stdsys (cant rmb which one)\n",
    "        if pbar is not None:\n",
    "            pbar.clear()\n",
    "            pbar.close()\n",
    "\n",
    "        # clear tensors from GPU memory to\n",
    "        # prevent memory leak\n",
    "        if attacked_data is not None:\n",
    "            attacked_data.to(\"cpu\")\n",
    "            del attacked_data\n",
    "            print(\"Cleared attacked data\")\n",
    "\n",
    "        if attack_stack is not None:\n",
    "            attack_stack.to(\"cpu\")\n",
    "            del attack_stack\n",
    "            print(\"Cleared attack stack\")\n",
    "\n",
    "        if mel is not None:\n",
    "            mel.to(\"cpu\")\n",
    "            del mel\n",
    "            print(\"Cleared mel\")\n",
    "\n",
    "        if logits is not None:\n",
    "            logits.to(\"cpu\")\n",
    "            del logits\n",
    "            print(\"Cleared logits\")\n",
    "\n",
    "        loss.to(\"cpu\")\n",
    "        del loss\n",
    "        print(\"Cleared loss\")\n",
    "\n",
    "        # empty GPU cache and garbage collect\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print_cuda_usage()\n",
    "        \n",
    "        return best_snippet.detach().to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JML5ybTKPMEu",
    "outputId": "f3650f0c-d671-4acf-ae9f-40bc0e620d18",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.902857780456543 GB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print_cuda_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jTU811FhsXQn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "PATIENCE = 3\n",
    "MIN_LIMIT = 45\n",
    "ITER_LIMIT = 30\n",
    "CLAMP_EP = 0.1\n",
    "PREPARE_METHOD = PrepareOverlayFront((1, 16000))\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard writer\n",
    "timestamp = datetime.datetime.now().strftime(f'%Y%m%d-%H%M%S_clamp_{CLAMP_EP}_{PREPARE_METHOD.name}')\n",
    "writer = SummaryWriter(log_dir=f\"runs/clamp_tests/{timestamp}\", max_queue=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfBzfVKNgHYp",
    "outputId": "80ab88a8-906c-4d44-a477-98b8717e3fc2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare method: prepare_overlay_front\n",
      "Snippet initialised to [8.406781489611603e-06, 0.09999900311231613] of size (1, 16000)\n",
      "Clamp: 0.1\n",
      "Time Limit (Mins): 45\n",
      "Epochs Limit: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [02:24<?, ?it/s, Iter 2, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 9.10246753692627 | Valid Avg Loss: 8.110187530517578 | Patience: 3 | LR: [0.001] | Epoch Limit: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [03:39<?, ?it/s, Iter 3, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 8.052210807800293 | Valid Avg Loss: 7.503369331359863 | Patience: 3 | LR: [0.001] | Epoch Limit: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [04:54<?, ?it/s, Iter 4, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 7.424419403076172 | Valid Avg Loss: 6.7608642578125 | Patience: 3 | LR: [0.001] | Epoch Limit: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [06:08<?, ?it/s, Iter 5, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.8995680809021 | Valid Avg Loss: 6.392514705657959 | Patience: 3 | LR: [0.001] | Epoch Limit: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [07:20<?, ?it/s, Iter 6, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.52614688873291 | Valid Avg Loss: 5.981173038482666 | Patience: 3 | LR: [0.001] | Epoch Limit: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [09:42<?, ?it/s, Iter 7, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.087812900543213 | Valid Avg Loss: 5.499251365661621 | Patience: 3 | LR: [0.0005] | Epoch Limit: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [11:09<?, ?it/s, Iter 8, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.838422775268555 | Valid Avg Loss: 5.300800323486328 | Patience: 3 | LR: [0.0005] | Epoch Limit: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [12:28<?, ?it/s, Iter 9, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.674785137176514 | Valid Avg Loss: 5.193559646606445 | Patience: 3 | LR: [0.0005] | Epoch Limit: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [13:49<?, ?it/s, Iter 10, Training Batch 2/500] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.518050670623779 | Valid Avg Loss: 5.032517433166504 | Patience: 3 | LR: [0.0005] | Epoch Limit: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [15:14<?, ?it/s, Iter 11, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.387734413146973 | Valid Avg Loss: 4.924654006958008 | Patience: 3 | LR: [0.0005] | Epoch Limit: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [16:48<?, ?it/s, Iter 12, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.221367359161377 | Valid Avg Loss: 4.776308059692383 | Patience: 3 | LR: [0.00025] | Epoch Limit: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [18:24<?, ?it/s, Iter 13, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.1237382888793945 | Valid Avg Loss: 4.7188262939453125 | Patience: 3 | LR: [0.00025] | Epoch Limit: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [19:38<?, ?it/s, Iter 14, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.056647777557373 | Valid Avg Loss: 4.633916854858398 | Patience: 3 | LR: [0.00025] | Epoch Limit: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [20:50<?, ?it/s, Iter 15, Training Batch 2/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 4.9959893226623535 | Valid Avg Loss: 4.573119163513184 | Patience: 3 | LR: [0.00025] | Epoch Limit: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [22:14<?, ?it/s, Iter 16, Training Batch 1/500]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 4.938578128814697 | Valid Avg Loss: 4.514774322509766 | Patience: 3 | LR: [0.00025] | Epoch Limit: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [23:55<?, ?it/s, Iter 16, Training Batch 425/500]"
     ]
    }
   ],
   "source": [
    "snippet = training_autograd(model, train_dataset, validation_dataset, \n",
    "                            PREPARE_METHOD,\n",
    "                            writer, lr=LR, \n",
    "                            iter_limit=ITER_LIMIT, mins_limit=MIN_LIMIT, patience=PATIENCE, clamp_epsilon=CLAMP_EP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q59EbCsIfr-9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_mel(snippet.detach().to(\"cpu\").squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-kaeGzDCEUV"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "uYCZCjkitb0J",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clamp: 0.02\n",
      "Prepare Method: prepare_overlay_front\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 250/250 [01:48<00:00,  2.30it/s, Valid Examples: 194 | Empty Sequences: 0 | Total SL = 23835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total valid examples: 194\n",
      "Success rate (Empty): 0.0\n",
      "Success rate (ASL): 122.86082474226804 (attacked) out of 122.16494845360825 (original)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "# for now, metric is whether the transcription is empty or consists only of blank tokens\n",
    "\n",
    "def evaluate(snippet, prepare_method, model, test_dataset):\n",
    "    print(f\"Clamp: {CLAMP_EP}\\nPrepare Method: {prepare_method.name}\")\n",
    "    empty_counter = 0\n",
    "    char_counter = 0\n",
    "    total_examples = 0\n",
    "    original_chars = 0\n",
    "\n",
    "    snippet = snippet.to(device)\n",
    "    pbar = tqdm(range(len(test_dataset)), desc=\"Inference\")\n",
    "    test_dataset_iter = iter(test_dataset)\n",
    "    model.eval()\n",
    "\n",
    "    for i in pbar:\n",
    "        # evaluate if there are any words at all\n",
    "        example, answer = next(test_dataset_iter).values()\n",
    "        if isinstance(answer, tuple) or isinstance(answer, list):\n",
    "            answer = answer[0]\n",
    "        if answer != \"ignore_time_segment_in_scoring\":\n",
    "            attacked_example = prepare_method(snippet, example.to(device))\n",
    "            transcription = model.transcribe(attacked_example.squeeze(), language=\"en\", condition_on_previous_text=False, fp16=True)[\"text\"]\n",
    "\n",
    "            if not transcription.strip():\n",
    "                empty_counter += 1\n",
    "            char_counter += len(transcription.strip())\n",
    "            original_chars += len(answer)\n",
    "            total_examples += 1\n",
    "            pbar.set_postfix_str(f\"Valid Examples: {total_examples} | Empty Sequences: {empty_counter} | Total SL = {char_counter}\")\n",
    "\n",
    "        example.to(\"cpu\")\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"\\n\")\n",
    "    print(f\"Total valid examples: {total_examples}\")\n",
    "    print(f\"Success rate (Empty): {empty_counter/total_examples}\")\n",
    "    print(f\"Success rate (ASL): {char_counter/total_examples} (attacked) out of {original_chars/total_examples} (original)\")\n",
    "\n",
    "evaluate(snippet, PREPARE_METHOD, model, test_dataset) # commented to prevent the runtime from autorunning and crashing the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalise(random_snippet, ep):\n",
    "    # we assume torch.rand inits to [0, 1)\n",
    "    res = random_snippet * ep * 2 - ep\n",
    "    print(f\"Normalised, Min {torch.min(res)}, Max {torch.max(res)}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hVIWKQO1V22"
   },
   "source": [
    "# Save and Hear Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pD8MUV-y1Rsj"
   },
   "outputs": [],
   "source": [
    "# Save snippet to wav file\n",
    "save_audio(snippet, f\"./snippets/clamp_{CLAMP_EP}_{PREPARE_METHOD.name}_snippet_only.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "xTI3OCNw5681"
   },
   "outputs": [],
   "source": [
    "save_audio(PREPARE_METHOD(snippet.to(\"cpu\"), tedlium_test[2][\"audio\"].unsqueeze(0)), f\"./snippets/clamp_{CLAMP_EP}_{PREPARE_METHOD.name}_combined.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
