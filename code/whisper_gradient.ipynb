{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5thx0SEBIIN"
   },
   "source": [
    "# Initialisation and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "lib_path = '/home/jovyan/libs'\n",
    "sys.path.insert(0, lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9rvOFi6QJP0z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc, math, traceback, datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import whisper\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "\n",
    "from utils import audio, gradient, gpu\n",
    "from utils.attacks import PrepareFront, PrepareAtPosition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EjUegebBNcX"
   },
   "source": [
    "# GPU RAM Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = gpu.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNEPbtquUG_g"
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aTrPYXKiygaU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    print(\"Model deleted!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zfgWJdLVJRbS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/libs/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"tiny.en\"\n",
    "\n",
    "model = whisper.load_model(MODEL_NAME).to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8P5xS3UBTfT"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "upnfsCvahcEJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tedlium_path = \"../tedlium\"\n",
    "train_path, validation_path, test_path = f\"{tedlium_path}/train_idx.hf\", f\"{tedlium_path}/validation_idx.hf\", f\"{tedlium_path}/test.hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d065452681c74b17925391762356c762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_SELECT = 500\n",
    "VALID_SELECT = 150\n",
    "TEST_SELECT = 250\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "tedlium_train = load_from_disk(train_path).with_format(\"torch\").shuffle(seed=SEED).select(range(TRAIN_SELECT))\n",
    "tedlium_validation = load_from_disk(validation_path).with_format(\"torch\").shuffle(seed=SEED).select(range(VALID_SELECT))\n",
    "tedlium_test = load_from_disk(test_path).with_format(\"torch\").shuffle(seed=SEED).select(range(TEST_SELECT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MoAt_Utrb4sT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def collate(ls):\n",
    "#     pad_to = max(list(map(lambda x: x[\"audio\"].shape[0], ls)))\n",
    "#     return torch.cat(list(map(lambda x: F.pad(x[\"audio\"], (0, pad_to - x[\"audio\"].shape[0])).unsqueeze(0).to(torch.bfloat16), ls)), dim=0)\n",
    "\n",
    "def collate_idx(ls):\n",
    "    return ls[0][\"audio\"].unsqueeze(0), ls[0][\"idx\"].item()\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1 # highly recommended to be 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "\n",
    "train_dataset = DataLoader(tedlium_train, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_idx)\n",
    "validation_dataset = DataLoader(tedlium_validation, batch_size=VALID_BATCH_SIZE, collate_fn=collate_idx)\n",
    "test_dataset = DataLoader(tedlium_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEl7cH9fBbZb"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JML5ybTKPMEu",
    "outputId": "f3650f0c-d671-4acf-ae9f-40bc0e620d18",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14174842834472656 GB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gpu.print_cuda_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jTU811FhsXQn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "PATIENCE = 5\n",
    "MIN_LIMIT = 45\n",
    "ITER_LIMIT = 30\n",
    "CLAMP_EP = 0.005\n",
    "SNIPPET_SIZE = (1, 6_800)\n",
    "POSITION = 16_000\n",
    "PREPARE_METHOD = PrepareAtPosition(SNIPPET_SIZE, POSITION)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard writer\n",
    "timestamp = datetime.datetime.now().strftime(f'%Y%m%d-%H%M%S_size_{SNIPPET_SIZE}_{PREPARE_METHOD.name}')\n",
    "writer = SummaryWriter(log_dir=f\"../runs/size_tests/{timestamp}\", max_queue=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfBzfVKNgHYp",
    "outputId": "80ab88a8-906c-4d44-a477-98b8717e3fc2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare method: prepare_at_position\n",
      "Snippet initialised to [3.107941438429407e-06, 0.004998547025024891] of size (1, 6800)\n",
      "Clamp: 0.005\n",
      "Time Limit (Mins): 45\n",
      "Epochs Limit: 30\n",
      "Tracking training success: False\n",
      "Tracking valid success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [03:01<?, ?it/s, Iter 1, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 7.948866844177246 | Valid Avg Loss: 8.048896789550781 | Patience: 5 | LR: [0.001] | Epoch Limit: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [05:05<?, ?it/s, Iter 2, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 7.6776909828186035 | Valid Avg Loss: 7.407385349273682 | Patience: 5 | LR: [0.001] | Epoch Limit: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [06:56<?, ?it/s, Iter 3, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 7.0080132484436035 | Valid Avg Loss: 7.173605918884277 | Patience: 5 | LR: [0.001] | Epoch Limit: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [08:28<?, ?it/s, Iter 4, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.775969982147217 | Valid Avg Loss: 7.016367435455322 | Patience: 5 | LR: [0.001] | Epoch Limit: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [09:30<?, ?it/s, Iter 5, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.901845455169678 | Valid Avg Loss: 7.114048004150391 | Patience: 4 | LR: [0.001] | Epoch Limit: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [10:33<?, ?it/s, Iter 6, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.0824174880981445 | Valid Avg Loss: 6.17254114151001 | Patience: 5 | LR: [0.0005] | Epoch Limit: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [11:36<?, ?it/s, Iter 7, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.6127705574035645 | Valid Avg Loss: 5.709960460662842 | Patience: 5 | LR: [0.0005] | Epoch Limit: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [12:40<?, ?it/s, Iter 8, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.325212001800537 | Valid Avg Loss: 5.983372688293457 | Patience: 4 | LR: [0.0005] | Epoch Limit: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [13:43<?, ?it/s, Iter 10, Training Batch 2/500]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.2271037101745605 | Valid Avg Loss: 5.970066070556641 | Patience: 3 | LR: [0.0005] | Epoch Limit: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [14:45<?, ?it/s, Iter 10, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.02964973449707 | Valid Avg Loss: 5.622575283050537 | Patience: 5 | LR: [0.0005] | Epoch Limit: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [16:35<?, ?it/s, Iter 11, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 3.139237642288208 | Valid Avg Loss: 3.7960007190704346 | Patience: 5 | LR: [0.00025] | Epoch Limit: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [18:40<?, ?it/s, Iter 12, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 2.4630630016326904 | Valid Avg Loss: 3.3148787021636963 | Patience: 5 | LR: [0.00025] | Epoch Limit: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [20:49<?, ?it/s, Iter 14, Training Batch 2/500]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 2.083742618560791 | Valid Avg Loss: 2.9950034618377686 | Patience: 5 | LR: [0.00025] | Epoch Limit: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [23:01<?, ?it/s, Iter 14, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.9601311683654785 | Valid Avg Loss: 2.6071858406066895 | Patience: 5 | LR: [0.00025] | Epoch Limit: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [24:57<?, ?it/s, Iter 15, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.6863411664962769 | Valid Avg Loss: 2.1483376026153564 | Patience: 5 | LR: [0.00025] | Epoch Limit: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [27:04<?, ?it/s, Iter 17, Training Batch 1/500]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.8921125531196594 | Valid Avg Loss: 1.3239494562149048 | Patience: 5 | LR: [0.000125] | Epoch Limit: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [29:10<?, ?it/s, Iter 17, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.7668418884277344 | Valid Avg Loss: 1.3484176397323608 | Patience: 4 | LR: [0.000125] | Epoch Limit: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [30:19<?, ?it/s, Iter 18, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.750175416469574 | Valid Avg Loss: 1.3023782968521118 | Patience: 5 | LR: [0.000125] | Epoch Limit: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [31:33<?, ?it/s, Iter 19, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.6418514847755432 | Valid Avg Loss: 0.8784810304641724 | Patience: 5 | LR: [0.000125] | Epoch Limit: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [33:25<?, ?it/s, Iter 20, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.49461430311203003 | Valid Avg Loss: 0.8934996724128723 | Patience: 4 | LR: [0.000125] | Epoch Limit: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [34:48<?, ?it/s, Iter 21, Validation Batch 150/150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.28399837017059326 | Valid Avg Loss: 0.5657040476799011 | Patience: 5 | LR: [6.25e-05] | Epoch Limit: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [36:55<?, ?it/s, Iter 23, Training Batch 1/500]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.2702611982822418 | Valid Avg Loss: 0.5914958715438843 | Patience: 4 | LR: [6.25e-05] | Epoch Limit: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [38:22<?, ?it/s, Iter 23, Training Batch 468/500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared buffer\n",
      "Cleared loss\n",
      "0.2993779182434082 GB\n"
     ]
    }
   ],
   "source": [
    "best_snippet, snippets, _, _ = gradient.train(model, train_dataset, validation_dataset, \n",
    "                                                            PREPARE_METHOD,\n",
    "                                                            writer, lr=LR, \n",
    "                                                            train_success=None, valid_success=None,\n",
    "                                                            iter_limit=ITER_LIMIT, mins_limit=MIN_LIMIT, patience=PATIENCE, clamp_epsilon=CLAMP_EP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q59EbCsIfr-9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio.view_mel(best_snippet.detach().to(\"cpu\").squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-kaeGzDCEUV"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "uYCZCjkitb0J",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils.gradient failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/jovyan/code/utils/gradient.py\", line 257, in <module>\n",
      "    def evaluate(model: whisper.model.Whisper, snippet: Tensor, prepare_method: PrepareMethod, test_dataset: Dataset, clamp_ep: float, position: tuple):\n",
      "NameError: name 'Dataset' is not defined\n",
      "]\n",
      "[autoreload of utils.gradient failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/jovyan/code/utils/gradient.py\", line 257, in <module>\n",
      "    def evaluate(model: whisper.model.Whisper, snippet: Tensor, prepare_method: PrepareMethod, test_dataset: Dataset, clamp_ep: float, position: tuple):\n",
      "NameError: name 'Dataset' is not defined\n",
      "]\n",
      "[autoreload of utils.gradient failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/jovyan/code/utils/gradient.py\", line 257, in <module>\n",
      "    def evaluate(model: whisper.model.Whisper, snippet: Tensor, prepare_method: PrepareMethod, test_dataset: Dataset, clamp_ep: float, position: tuple):\n",
      "NameError: name 'Dataset' is not defined\n",
      "]\n",
      "[autoreload of utils.gradient failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/jovyan/code/utils/gradient.py\", line 257, in <module>\n",
      "    def evaluate(model: whisper.model.Whisper, snippet: Tensor, prepare_method: PrepareMethod, test_dataset: Dataset, clamp_ep: float, position: tuple):\n",
      "NameError: name 'Dataset' is not defined\n",
      "]\n",
      "[autoreload of utils.gradient failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/jovyan/code/utils/gradient.py\", line 257, in <module>\n",
      "    def evaluate(model: whisper.model.Whisper, snippet: Tensor, prepare_method: PrepareMethod, test_dataset: Dataset, clamp_ep: float, position: tuple):\n",
      "NameError: name 'Dataset' is not defined\n",
      "]\n",
      "[autoreload of utils.gradient failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/jovyan/code/utils/gradient.py\", line 257, in <module>\n",
      "    def evaluate(model: whisper.model.Whisper, snippet: Tensor, prepare_method: PrepareMethod, test_dataset: Dataset, clamp_ep: float, position: tuple):\n",
      "NameError: name 'Dataset' is not defined\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clamp: 0.005\n",
      "Prepare Method: prepare_at_position\n",
      "Snippet Size: (1, 6800)\n",
      "Position: 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 250/250 [01:10<00:00,  3.56it/s, Valid Examples: 194 | Empty Sequences: 113 | Total SL = 9166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total valid examples: 194\n",
      "Success rate (Empty): 0.5824742268041238\n",
      "Success rate (ASL): 47.24742268041237 (attacked) out of 122.16494845360825 (original)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gradient.evaluate(best_snippet, PREPARE_METHOD, model, test_dataset, CLAMP_EP, POSITION) # commented to prevent the runtime from autorunning and crashing the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snippets = torch.stack(list(map(lambda x: x.cpu(), snippets)) + [best_snippet.cpu()])\n",
    "snippets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(snippets.squeeze(), \"snippets.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(torch.stack(list(map(torch.tensor, train_success.values()))), \"train_success.pt\")\n",
    "torch.save(torch.tensor(list(train_success.keys())), \"train_ids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(torch.stack(list(map(torch.tensor, valid_success.values()))), \"valid_success.pt\")\n",
    "torch.save(torch.tensor(list(valid_success.keys())), \"valid_ids.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hVIWKQO1V22"
   },
   "source": [
    "# Save and Hear Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalise(random_snippet, ep):\n",
    "    # we assume torch.rand inits to [0, 1)\n",
    "    res = random_snippet * ep * 2 - ep\n",
    "    print(f\"Normalised, Min {torch.min(res)}, Max {torch.max(res)}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD8MUV-y1Rsj"
   },
   "outputs": [],
   "source": [
    "# Save snippet to wav file\n",
    "save_audio(snippet, f\"./snippets/clamp_{CLAMP_EP}_{PREPARE_METHOD.name}_snippet_only.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTI3OCNw5681"
   },
   "outputs": [],
   "source": [
    "save_audio(PREPARE_METHOD(snippet.to(\"cpu\"), tedlium_test[2][\"audio\"].unsqueeze(0)), f\"./snippets/clamp_{CLAMP_EP}_{PREPARE_METHOD.name}_combined.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
