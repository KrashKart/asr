{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5thx0SEBIIN"
   },
   "source": [
    "# Initialisation and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "lib_path = '/home/jovyan/libs'\n",
    "sys.path.insert(0, lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9rvOFi6QJP0z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.8/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc, math, traceback, datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import whisper\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "\n",
    "from utils import audio, gradient, gpu\n",
    "from utils.attacks import PrepareFront, PrepareAtPosition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EjUegebBNcX"
   },
   "source": [
    "# GPU RAM Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = gpu.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNEPbtquUG_g"
   },
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aTrPYXKiygaU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "    print(\"Model deleted!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zfgWJdLVJRbS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/libs/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "SMALL_MODEL = \"small.en\"\n",
    "TINY_MODEL = \"tiny.en\"\n",
    "\n",
    "small_model = whisper.load_model(SMALL_MODEL).to(device)\n",
    "small_model.eval()\n",
    "\n",
    "tiny_model = whisper.load_model(TINY_MODEL).to(device)\n",
    "tiny_model.eval()\n",
    "\n",
    "target_id, sot_ids = gradient._get_ids(small_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8P5xS3UBTfT"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "upnfsCvahcEJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tedlium_path = \"../tedlium\"\n",
    "train_path, validation_path, test_path = f\"{tedlium_path}/train_idx.hf\", f\"{tedlium_path}/validation_idx.hf\", f\"{tedlium_path}/test.hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e254db59b0443fadc9e7ef7d61e2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_SELECT = 250\n",
    "VALID_SELECT = 100\n",
    "TEST_SELECT = 150\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "tedlium_train = load_from_disk(train_path).with_format(\"torch\").shuffle(seed=SEED).select(range(TRAIN_SELECT))\n",
    "tedlium_validation = load_from_disk(validation_path).with_format(\"torch\").shuffle(seed=SEED).select(range(VALID_SELECT))\n",
    "tedlium_test = load_from_disk(test_path).with_format(\"torch\").shuffle(seed=SEED).select(range(TEST_SELECT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MoAt_Utrb4sT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def collate(ls):\n",
    "#     pad_to = max(list(map(lambda x: x[\"audio\"].shape[0], ls)))\n",
    "#     return torch.cat(list(map(lambda x: F.pad(x[\"audio\"], (0, pad_to - x[\"audio\"].shape[0])).unsqueeze(0).to(torch.bfloat16), ls)), dim=0)\n",
    "\n",
    "def collate_idx(ls):\n",
    "    return ls[0][\"audio\"].unsqueeze(0), ls[0][\"idx\"].item()\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1 # highly recommended to be 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "\n",
    "train_dataset = DataLoader(tedlium_train, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_idx)\n",
    "validation_dataset = DataLoader(tedlium_validation, batch_size=VALID_BATCH_SIZE, collate_fn=collate_idx)\n",
    "test_dataset = DataLoader(tedlium_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEl7cH9fBbZb"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JML5ybTKPMEu",
    "outputId": "f3650f0c-d671-4acf-ae9f-40bc0e620d18",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jTU811FhsXQn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "PATIENCE = 7\n",
    "MIN_LIMIT = None\n",
    "ITER_LIMIT = None\n",
    "CLAMP_EP = 0.005\n",
    "SNIPPET_SIZE = (1, 10_240)\n",
    "POSITION = 0\n",
    "PREPARE_METHOD = PrepareAtPosition(SNIPPET_SIZE, POSITION)\n",
    "DELTA = 0.01\n",
    "\n",
    "CLAMP_EPS = (0.00125, 0.0025, 0.005)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard writer\n",
    "# timestamp = datetime.datetime.now().strftime(f'%Y%m%d-%H%M%S_size_{SNIPPET_SIZE}_{PREPARE_METHOD.name}')\n",
    "# writer = SummaryWriter(log_dir=f\"../runs/size_tests/{timestamp}\", max_queue=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfBzfVKNgHYp",
    "outputId": "80ab88a8-906c-4d44-a477-98b8717e3fc2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/libs/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare method: prepare_at_position\n",
      "Snippet initialised to [1.0212062306891312e-06, 0.004999465774744749] of size (1, 10240)\n",
      "Clamp: 0.005\n",
      "Time Limit (Mins): None\n",
      "Epochs Limit: None\n",
      "Tracking training success: False\n",
      "Tracking valid success: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [02:49<?, ?it/s, Iter 2, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 7.408213442802429 | Valid Avg Loss: 7.685433864593506 | Patience: 7 | LR: [0.001] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [05:13<?, ?it/s, Iter 3, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 7.050932130813599 | Valid Avg Loss: 7.495156288146973 | Patience: 7 | LR: [0.001] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [07:39<?, ?it/s, Iter 4, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 7.549940779209137 | Valid Avg Loss: 7.170105457305908 | Patience: 7 | LR: [0.001] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [09:58<?, ?it/s, Iter 5, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.502966806888581 | Valid Avg Loss: 7.135991096496582 | Patience: 7 | LR: [0.001] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [12:17<?, ?it/s, Iter 6, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 6.67595689868927 | Valid Avg Loss: 7.142073154449463 | Patience: 6 | LR: [0.001] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [14:23<?, ?it/s, Iter 7, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.086055135250091 | Valid Avg Loss: 5.7306694984436035 | Patience: 7 | LR: [0.0005] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [16:30<?, ?it/s, Iter 8, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.157282932758331 | Valid Avg Loss: 5.015968322753906 | Patience: 7 | LR: [0.0005] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [18:42<?, ?it/s, Iter 9, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.485568301677704 | Valid Avg Loss: 6.522902011871338 | Patience: 6 | LR: [0.0005] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [20:48<?, ?it/s, Iter 10, Training Batch 1/250]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 5.055273509025573 | Valid Avg Loss: 5.493022918701172 | Patience: 5 | LR: [0.0005] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [22:51<?, ?it/s, Iter 11, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 4.834996129512787 | Valid Avg Loss: 4.82178258895874 | Patience: 7 | LR: [0.0005] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [24:36<?, ?it/s, Iter 12, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 3.4732904291152953 | Valid Avg Loss: 3.905441999435425 | Patience: 7 | LR: [0.00025] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [26:23<?, ?it/s, Iter 13, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 3.7926881642341614 | Valid Avg Loss: 3.9466350078582764 | Patience: 6 | LR: [0.00025] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [28:08<?, ?it/s, Iter 14, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 3.777814440727234 | Valid Avg Loss: 4.352321624755859 | Patience: 5 | LR: [0.00025] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [29:53<?, ?it/s, Iter 15, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 3.5098293986320495 | Valid Avg Loss: 3.863316059112549 | Patience: 7 | LR: [0.00025] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [31:28<?, ?it/s, Iter 16, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 3.18808256149292 | Valid Avg Loss: 2.8326191902160645 | Patience: 7 | LR: [0.00025] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [32:51<?, ?it/s, Iter 17, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 2.3053841490745546 | Valid Avg Loss: 3.0545878410339355 | Patience: 6 | LR: [0.000125] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [34:12<?, ?it/s, Iter 18, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 2.129672840595245 | Valid Avg Loss: 2.760840892791748 | Patience: 7 | LR: [0.000125] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [35:34<?, ?it/s, Iter 19, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 2.1550323598384855 | Valid Avg Loss: 2.462954521179199 | Patience: 7 | LR: [0.000125] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [36:53<?, ?it/s, Iter 20, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.9487013676166534 | Valid Avg Loss: 2.2845845222473145 | Patience: 7 | LR: [0.000125] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [38:08<?, ?it/s, Iter 21, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.7600070093870164 | Valid Avg Loss: 2.7867279052734375 | Patience: 6 | LR: [0.000125] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [39:22<?, ?it/s, Iter 22, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.5322322051525117 | Valid Avg Loss: 1.8358352184295654 | Patience: 7 | LR: [6.25e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [40:32<?, ?it/s, Iter 23, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.3157225685715674 | Valid Avg Loss: 1.2536004781723022 | Patience: 7 | LR: [6.25e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [41:37<?, ?it/s, Iter 24, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.0839357984662057 | Valid Avg Loss: 1.1307522058486938 | Patience: 7 | LR: [6.25e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [42:41<?, ?it/s, Iter 25, Training Batch 1/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 1.00020832580328 | Valid Avg Loss: 1.5069950819015503 | Patience: 6 | LR: [6.25e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [43:47<?, ?it/s, Iter 26, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.891886518239975 | Valid Avg Loss: 1.6240203380584717 | Patience: 5 | LR: [6.25e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [44:42<?, ?it/s, Iter 27, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.640330873399973 | Valid Avg Loss: 0.9296208620071411 | Patience: 7 | LR: [3.125e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [45:33<?, ?it/s, Iter 28, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.4922091069817543 | Valid Avg Loss: 0.9157963395118713 | Patience: 7 | LR: [3.125e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [46:25<?, ?it/s, Iter 29, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.5772376187443733 | Valid Avg Loss: 0.7145307064056396 | Patience: 7 | LR: [3.125e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [47:25<?, ?it/s, Iter 30, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.37000648303329947 | Valid Avg Loss: 0.8241088390350342 | Patience: 6 | LR: [3.125e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [48:20<?, ?it/s, Iter 31, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.5655862446576357 | Valid Avg Loss: 0.7333850860595703 | Patience: 5 | LR: [3.125e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [49:10<?, ?it/s, Iter 32, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.3468946526348591 | Valid Avg Loss: 0.7024179100990295 | Patience: 7 | LR: [1.5625e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [50:03<?, ?it/s, Iter 33, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.30378655920922754 | Valid Avg Loss: 0.6970689296722412 | Patience: 6 | LR: [1.5625e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [50:57<?, ?it/s, Iter 34, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.28705418469011784 | Valid Avg Loss: 0.6941412091255188 | Patience: 5 | LR: [1.5625e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [51:47<?, ?it/s, Iter 35, Training Batch 2/250]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trng Avg Loss: 0.28584949153661726 | Valid Avg Loss: 0.6941326260566711 | Patience: 4 | LR: [1.5625e-05] | Epoch Limit: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0% 0/1 [52:15<?, ?it/s, Iter 35, Training Batch 178/250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared buffer\n",
      "Cleared loss\n"
     ]
    }
   ],
   "source": [
    "best_snippet, snippets, _, _ = gradient.train(small_model, gradient.forward_multi,\n",
    "                                              train_dataset, validation_dataset,\n",
    "                                              PREPARE_METHOD,\n",
    "                                              writer, lr=LR,\n",
    "                                              train_success=None, valid_success=None,\n",
    "                                              iter_limit=ITER_LIMIT, mins_limit=MIN_LIMIT, patience=PATIENCE, clamp_epsilon=CLAMP_EP, gap=DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q59EbCsIfr-9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# audio.view_mel(best_snippet.detach().to(\"cpu\").squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-kaeGzDCEUV"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uYCZCjkitb0J",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clamp: 0.005\n",
      "Prepare Method: prepare_at_position\n",
      "Snippet Size: (1, 10240)\n",
      "Position: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 150/150 [01:03<00:00,  2.37it/s, Valid Examples: 120 | Empty Sequences: 27 | Total SL: 8283 | Non-empty ASL: 89.06451612903226 | Total Bleu Score: 41.109153747558594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total valid examples: 120\n",
      "Success rate (Empty): 0.225\n",
      "Success rate (ASL): 69.025 (attacked) out of 122.75833333333334 (original)\n",
      "Average Bleu Score: 0.3425762951374054\n",
      "Average WER: 0.6654056953407352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gradient.evaluate(small_model, best_snippet, PREPARE_METHOD, test_dataset, CLAMP_EP, POSITION) # commented to prevent the runtime from autorunning and crashing the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_snippet = (torch.rand(SNIPPET_SIZE) - 0.5) * (CLAMP_EP / 0.5)\n",
    "# print(torch.max(random_snippet), torch.min(random_snippet))\n",
    "# gradient.evaluate(model, random_snippet, PREPARE_METHOD, test_dataset, CLAMP_EP, POSITION) # commented to prevent the runtime from autorunning and crashing the thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for C in (0.005,):\n",
    "#     print(C)\n",
    "#     best_snippet, _, _, _ = gradient.train(model, gradient.forward_auto,\n",
    "#                                               train_dataset, validation_dataset,\n",
    "#                                               PREPARE_METHOD,\n",
    "#                                               writer, lr=LR, \n",
    "#                                               train_success=None, valid_success=None,\n",
    "#                                               iter_limit=ITER_LIMIT, mins_limit=MIN_LIMIT, patience=PATIENCE, clamp_epsilon=C, gap=DELTA)\n",
    "#     gradient.evaluate(model, best_snippet, PREPARE_METHOD, test_dataset, C, POSITION) # commented to prevent the runtime from autorunning and crashing the thing\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LENGTHS = (1600, 3200, 4800, 6400, 6800, 7200, 8000, 10240, 16000)\n",
    "\n",
    "# for L in LENGTHS:\n",
    "#     print(L)\n",
    "#     PREPARE_METHOD = PrepareAtPosition((1, L), 0)\n",
    "#     best_snippet, _, _, _ = gradient.train(model, gradient.forward_auto,\n",
    "#                                               train_dataset, validation_dataset,\n",
    "#                                               PREPARE_METHOD,\n",
    "#                                               writer, lr=LR, \n",
    "#                                               train_success=None, valid_success=None,\n",
    "#                                               iter_limit=ITER_LIMIT, mins_limit=MIN_LIMIT, patience=PATIENCE, clamp_epsilon=CLAMP_EP, gap=DELTA)\n",
    "#     gradient.evaluate(model, best_snippet, PREPARE_METHOD, test_dataset, CLAMP_EP, POSITION) # commented to prevent the runtime from autorunning and crashing the thing\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 1, 10240])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippets = torch.stack(list(map(lambda x: x.cpu(), snippets)) + [best_snippet.cpu()])\n",
    "snippets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(snippets.squeeze(), \"snippets.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(torch.stack(list(map(torch.tensor, train_success.values()))), \"train_success.pt\")\n",
    "# torch.save(torch.tensor(list(train_success.keys())), \"train_ids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(torch.stack(list(map(torch.tensor, valid_success.values()))), \"valid_success.pt\")\n",
    "# torch.save(torch.tensor(list(valid_success.keys())), \"valid_ids.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hVIWKQO1V22"
   },
   "source": [
    "# Save and Hear Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def normalise(random_snippet, ep):\n",
    "#     # we assume torch.rand inits to [0, 1)\n",
    "#     res = random_snippet * ep * 2 - ep\n",
    "#     print(f\"Normalised, Min {torch.min(res)}, Max {torch.max(res)}\")\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pD8MUV-y1Rsj"
   },
   "outputs": [],
   "source": [
    "# Save snippet to wav file\n",
    "# save_audio(snippet, f\"./snippets/clamp_{CLAMP_EP}_{PREPARE_METHOD.name}_snippet_only.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xTI3OCNw5681"
   },
   "outputs": [],
   "source": [
    "# save_audio(PREPARE_METHOD(snippet.to(\"cpu\"), tedlium_test[2][\"audio\"].unsqueeze(0)), f\"./snippets/clamp_{CLAMP_EP}_{PREPARE_METHOD.name}_combined.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
